# Introduction to this script----
# This script accomplishes several goals:
# 1st - it allows us to read in and modify our reference transcriptome fastq file (only needs to be done once)
# 2nd - it uses Sleuth to identify differentially expressed genes directly from the Kallisto output
# 3rd - this script introduces us to basic tools for annotation
# 4th - allows us read our transcript abundance counts (from Kallisto) directly into R

# Load packages ----
library(tidyverse) # provides access to Hadley Wickham's collection of R packages for data science, which we will use throughout the course
library(Biostrings) # we'll use this package to read in and modify our reference fasta file
library(tximport) # package for getting Kallisto results into R
library(ensembldb) # used together with our organism-specific database package (below) to get annotation info
library(EnsDb.Hsapiens.v86) # organism-specific database package
library(sleuth) #for rapid and simple differential gene expression analysis

# OPTIONAL: modify reference transcriptome .fasta file ----
# depending on your reference transcriptome, we may need to edit the fasta file (but this would only need to be done once)
# Transcript identifiers should NOT have a version number appended to the ID
# this creats a problem later when we try to append annotations based on these IDs
# you would only need to edit this file once
Hs.trans <- readDNAStringSet("Homo_sapiens.GRCh38.cdna.all.fa")
slotNames(Hs.trans) #look at the 'slots' in the object you just created
Hs.trans@ranges@NAMES #access one of these slots
#replace transcript names with new names that don't have decimal version number
names(Hs.trans) <- gsub("(\\.).*", "", names(Hs.trans)) #the gsub command is one of many 'grep' functions included in baseR
writeXStringSet(Hs.trans, "Hs.trans.shortnames.fasta")

# Differential transcript analysis using Sleuth ----
# If doing this with your own data, be sure to:
# 1. make the first column of your study design file is called 'sample' and matches the names of your kallisto ouput folders
# 2. if you used Salmon instead of Kallisto for read mapping, you can still use Sleuth but you'll need the Wasabi R package

#read in your study design ---- 
#there are LOTS of ways to read data into R, but the readr package (from tidyverse) is one of the simplest
targets <- read_tsv("Crypto_studyDesign.txt")
# you can easily create file paths to the abundance files generated by Kallisto using the 'file.path' function
path <- file.path(targets$sample, "abundance.h5")
# now check to make sure this path is correct by seeing if the files exist
all(file.exists(path)) 
# this should look familiar from your homework
# the functions which(), any() and all() take logical vectors as their argument. The any()
# function will return TRUE if one or more of the elements in the logical vector is TRUE. The all() function
# will return TRUE if every element in the logical vector is TRUE.

# use dplyr to modify your study design to include these file paths as a new column.  
# Sleuth looks for this column, which must be called 'path', in order to locate the data output from Kallisto
targets <- mutate(targets, path)

# differential expression analysis using Sleuth ----
# Now you're ready to construct a sleuth object
mySleuth <- sleuth_prep(targets,  
                        #target_mapping = Tx, #uncomment this line if you want to map transcripts IDs to gene symbols
                        #aggregation_column = 'gene_name', #uncomment this line if you want to collapse your data to gene level
                        read_bootstrap_tpm=TRUE,
                        extra_bootstrap_summary=TRUE) 

# fit a linear model to the full data
mySleuth <- sleuth_fit(mySleuth, ~treatment2, 'full')
# for differential expression we must also fit a second 'reduced' model
mySleuth <- sleuth_fit(mySleuth, ~1, 'reduced')
# perform a likelihood ratio test (LRT) using the two models above
mySleuth <- sleuth_lrt(mySleuth, 'reduced', 'full')
# now launch an interactive shiny app to explore the results
sleuth_live(mySleuth)

# get annotations ----
# If we want to know what kinds of data are retriveable, 
# use EnsemblDB package functions to look at the tables of the annotation database
listTables(EnsDb.Hsapiens.v86)
# now take a look at the columns of the "tx" table within the database
listColumns(EnsDb.Hsapiens.v86, "tx")

#use the 'transcripts' function from the EnsemblDB package to get annotation info
Tx <- transcripts(EnsDb.Hsapiens.v86, 
                  columns=c(listColumns(EnsDb.Hsapiens.v86,
                                        "tx"), "gene_name"))

# use the tibble package (another part of the tidyverse) to convert the annotation info into a dataframe
Tx <- as.tibble(Tx)
head(Tx)

# need to rename the 'Tx_id' column name to 'target_id'
# again, this is because Sleuth expects this column name
Tx <- dplyr::rename(Tx, target_id = tx_id)
head(Tx)

#transcript ID needs to be the first column in the dataframe
Tx <- dplyr::select(Tx, target_id, gene_name)
head(Tx)

# now that you've captured basic gene symbol annotations for your ensembl transcript IDs, go back and rerun Sleuth analysis incorporating this information.

# OPTIONAL: Get annotations for other organisms using BiomaRt----
library(biomaRt)
listMarts() #default host is ensembl.org, and most current release of mammalian genomes
#listMarts(host="parasite.wormbase.org") #access to parasite worm genomes
#listMarts(host="protists.ensembl.org") #access to protozoan genomes

#choose the 'mart' you want to work with
myMart <- useMart(biomart="ENSEMBL_MART_ENSEMBL")
#take a look at all available datasets within the selected mart
available.datasets <- listDatasets(myMart)
#I'll use canine as an example
canFam <- useMart(biomart="ENSEMBL_MART_ENSEMBL", dataset = "cfamiliaris_gene_ensembl")
canFam.filters <- listFilters(canFam)

TxIDs_to_genes <- getBM(attributes=c('ensembl_transcript_id_version',
                                     #'ensembl_transcript_id',
                                     'external_gene_name'),
                        mart = canFam)

# Import Kallisto transcript counts into R using Tximport ----
# copy the abundance files to the working directory and rename so that each sample has a unique name
Txi_gene <- tximport(path, 
                     type = "kallisto", 
                     tx2gene = Tx, 
                     txOut = TRUE, #How does the result change if this =FALSE vs =TRUE?
                     countsFromAbundance = "lengthScaledTPM")

#take a look at the object you just created
head(Txi_gene$counts) # these are you counts after adjusting for transcript length
head(Txi_gene$abundance) # these are your transcript per million (TPM) values
names
attributes

#QUIZ----
#1. what should the columns of Txi_gene$abundance sum to, what about Txi_gene$counts?
#2. how would you quickly find out the answer to #1 in R?